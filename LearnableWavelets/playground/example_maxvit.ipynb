{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example script to run a learnable wavelet scattering network on a subset of the CAMELs data\n",
    "\n",
    "Simple example script. **Make sure you have run the `wget` command in the README to download the example data file**\n",
    "\n",
    "We take a small sample of the CAMELs dataset (1k $M_\\mathrm{tot}$ maps), and train a \"SN\" (2 layers of wavelet convolutions, 8 wavelet filters each, and pass the output to a CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelhealy\u001b[0m (\u001b[33mwaveletcapstone\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install wandb -qqq\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Learnable scattering modules\n",
    "from learnable_wavelets.models import models_factory, sn_hybrid_models, camels_models\n",
    "from learnable_wavelets.datasets import camels_dataset \n",
    "\n",
    "## MaxViT\n",
    "import timm\n",
    "\n",
    "from itertools import product\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training parameters\n",
    "batch_size  = 32\n",
    "num_workers = 10    #number of workers to load data\n",
    "lr_sn       = 0.01  ## Learning rate for scattering layers\n",
    "lr          = 0.005 ## Learning rate for neural weights\n",
    "wd          = 0.18   ## weight decay\n",
    "epochs      = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n"
     ]
    }
   ],
   "source": [
    "# use GPUs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "    use_cuda=True\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "    use_cuda=False\n",
    "cudnn.benchmark = True      #May train faster but cost more memory\n",
    "\n",
    "# architecture parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "fparams    = \"/scratch/cp3759/camels_data/params_IllustrisTNG.txt\"\n",
    "#fparams    = \"../test_data/params_IllustrisTNG.txt\" ## Simulation parameter file\n",
    "fmaps      = [\"/scratch/cp3759/camels_data/Maps_Mtot_IllustrisTNG_LH_z=0.00.npy\"]\n",
    "#fmaps      = [\"../test_data/maps_Mtot_1k.npy\"]   ## Simulated maps, must be a list as we can take multiple maps\n",
    "\n",
    "fmaps_norm      = [None]\n",
    "splits          = 15      ## Number of maps taken from each sim (range between 1 and 15, must match the dataset size)\n",
    "                         ## i.e. splits=1 for 1k maps, splits=15 for 15k maps, or any integer value in between\n",
    "seed            = 123    ## seed for the test/valid/train split\n",
    "monopole        = True   ## Keep the monopole of the maps (True) or remove it (False)\n",
    "rot_flip_in_mem = False  ## Whether rotations and flipings are kept in memory (faster but takes more memory if true)\n",
    "smoothing       = 0      ## Smooth the maps with a Gaussian filter? 0 for no\n",
    "arch            = \"maxvit\"   ## Which model architecture to use\n",
    "features        = 4    ## Number of variables to train the model on. This can be 2, 4, 6 or 12, depending on whether\n",
    "                         ## you want to a) train on both cosmological and IGM parameters and b) also ask the\n",
    "                         ## network to estimate uncertanties on these parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Load and plot a random map\n",
    "maps = np.load(fmaps[0])\n",
    "maps=maps[np.random.randint(len(maps))]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(np.log10(maps))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=len(fmaps)\n",
    "## Set up indices to use for the loss function\n",
    "if features==2:\n",
    "    g=[0,1]\n",
    "if features==4:\n",
    "    g=[0,1]\n",
    "    h=[2,3]\n",
    "elif features==6:\n",
    "    g=[0,1,2,3,4,5]\n",
    "elif features==12:\n",
    "    g=[0,1,2,3,4,5]\n",
    "    h=[6,7,8,9,10,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelhealy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/mbh425/LearnableWavelets/playground/wandb/run-20221127_072130-2nzvnvzy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/michaelhealy/Capstone%20Maxvit/runs/2nzvnvzy\" target=\"_blank\">devoted-dream-10</a></strong> to <a href=\"https://wandb.ai/michaelhealy/Capstone%20Maxvit\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/michaelhealy/Capstone%20Maxvit/runs/2nzvnvzy?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x15402795b130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"learning rate\": lr,\n",
    "            \"scattering learning rate\": lr_sn,\n",
    "            \"wd\": wd,\n",
    "            \"channels\": channels,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch size\": batch_size,\n",
    "            \"network\": arch,\n",
    "            \"features\": features,\n",
    "            \"splits\":splits}\n",
    "\n",
    "wandb.init(project=\"Capstone Maxvit\", entity=\"michaelhealy\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing training set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "4.598e+09 < F(all|orig) < 3.186e+15\n",
      "9.663 < F(all|resc)  < 15.503\n",
      "-2.931 < F(all|norm) < 8.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing validation set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "4.598e+09 < F(all|orig) < 3.186e+15\n",
      "9.663 < F(all|resc)  < 15.503\n",
      "-2.931 < F(all|norm) < 8.946\n",
      "\n",
      "Preparing test set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "4.598e+09 < F(all|orig) < 3.186e+15\n",
      "9.663 < F(all|resc)  < 15.503\n",
      "-2.931 < F(all|norm) < 8.946\n"
     ]
    }
   ],
   "source": [
    "## Generate torch datasets\n",
    "print('\\nPreparing training set')\n",
    "train_loader = camels_dataset.create_dataset_multifield('train', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                         num_workers=num_workers, rot_flip_in_mem=rot_flip_in_mem, verbose=True)\n",
    "\n",
    "# get validation set\n",
    "print('\\nPreparing validation set')\n",
    "valid_loader = camels_dataset.create_dataset_multifield('valid', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                         num_workers=num_workers, rot_flip_in_mem=rot_flip_in_mem,  verbose=True)    \n",
    "\n",
    "# get test set\n",
    "print('\\nPreparing test set')\n",
    "test_loader = camels_dataset.create_dataset_multifield('test', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                        num_workers=num_workers, rot_flip_in_mem=rot_flip_in_mem,  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_maps=len(train_loader.dataset.x)\n",
    "wandb.config.update({\"no. training maps\": num_train_maps,\n",
    "                        \"fields\": fmaps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxViT Base model created\n"
     ]
    }
   ],
   "source": [
    "#Create model\n",
    "model_size = 'Base'\n",
    "data_size = '15k'\n",
    "#model = timm.models.maxvit_tiny_224(in_chans=channels,num_classes=features)\n",
    "model = timm.models.maxvit_base_224(in_chans=channels,num_classes=features)\n",
    "#model = timm.models.maxxvit_rmlp_tiny_rw_256(in_chans=channels,num_classes=features)\n",
    "wandb.config.update({\"learnable_parameters\":sum(p.numel() for p in model.parameters())})\n",
    "model.to(device=device) ## Put model on the appropriate device\n",
    "print(f\"MaxViT {model_size} model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below counts trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in tiny = 30141644\n",
      "Number of trainable parameters in small = 67570160\n",
      "Number of trainable parameters in base = 118109912\n",
      "Number of trainable parameters in large = 209712500\n",
      "Number of trainable parameters in xlarge = 471056428\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Number of trainable parameters in tiny =',count_parameters(timm.models.maxvit_tiny_224(in_chans=channels,num_classes=features)))\n",
    "print('Number of trainable parameters in small =',count_parameters(timm.models.maxvit_small_224(in_chans=channels,num_classes=features)))\n",
    "print('Number of trainable parameters in base =',count_parameters(timm.models.maxvit_base_224(in_chans=channels,num_classes=features)))\n",
    "print('Number of trainable parameters in large =',count_parameters(timm.models.maxvit_large_224(in_chans=channels,num_classes=features)))\n",
    "print('Number of trainable parameters in xlarge =',count_parameters(timm.models.maxvit_xlarge_224(in_chans=channels,num_classes=features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Model Loop Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log_freq=1)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 6.955e+00 -5.311e+00 \n",
      "001 -9.376e+00 -7.834e+00 \n"
     ]
    }
   ],
   "source": [
    "## Train and valid loops\n",
    "start = time.time()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs): #edited to slow down\n",
    "    log_dic={}\n",
    "    # train\n",
    "    train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for x, y in train_loader:\n",
    "        x    = x[:,:,16:-16,16:-16]\n",
    "        bs   = x.shape[0]        #batch size\n",
    "        x    = x.to(device)       #maps\n",
    "        y    = y.to(device)[:,g]  #parameters\n",
    "        i+=1\n",
    "        #print(f'Training:    Epoch={epoch}    Iteration={i}')\n",
    "        p    = model(x)           #NN output\n",
    "        y_NN = p[:,g]             #posterior mean\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        if features==4 or features==12:\n",
    "            e_NN = p[:,h]         #posterior std\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "            train_loss2 += loss2*bs\n",
    "        else:\n",
    "            loss = torch.mean(torch.log(loss1))\n",
    "        train_loss1 += loss1*bs\n",
    "        points      += bs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = torch.log(train_loss1/points) \n",
    "    if features==4 or features==12:\n",
    "        train_loss+=torch.log(train_loss2/points)\n",
    "    train_loss = torch.mean(train_loss).item()\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    # do validation: cosmo alone & all params\n",
    "    valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    valid_loss, points = 0.0, 0\n",
    "    model.eval()\n",
    "    for x, y in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            x    = x[:,:,16:-16,16:-16]\n",
    "            bs    = x.shape[0]         #batch size\n",
    "            x     = x.to(device)       #maps\n",
    "            y     = y.to(device)[:,g]  #parameters\n",
    "            i+=1\n",
    "            #print(f'Validation:    Epoch={epoch}    Iteration={i}')\n",
    "            p     = model(x)           #NN output\n",
    "            y_NN  = p[:,g]             #posterior mean\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            if features==4 or features==12:    \n",
    "                e_NN  = p[:,h]         #posterior std\n",
    "                loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                valid_loss2 += loss2*bs\n",
    "            valid_loss1 += loss1*bs\n",
    "            points     += bs\n",
    "\n",
    "\n",
    "    valid_loss = torch.log(valid_loss1/points) \n",
    "    if features==4 or features==12:\n",
    "        valid_loss+=torch.log(valid_loss2/points)\n",
    "    valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "    log_dic[\"training_loss\"]=train_loss\n",
    "    log_dic[\"valid_loss\"]=valid_loss\n",
    "    wandb.log(log_dic)\n",
    "\n",
    "    # verbose\n",
    "    print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "    print(\"\")\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)\n",
    "\n",
    "stop = time.time()\n",
    "print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))\n",
    "\n",
    "## Model performance metrics on test set\n",
    "num_maps=test_loader.dataset.size\n",
    "## Now loop over test set and print accuracy\n",
    "# define the arrays containing the value of the parameters\n",
    "params_true = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "params_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "errors_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*[range(epochs)],train_losses,label = 'Training Loss', linestyle=\"-.\")\n",
    "plt.plot(*[range(epochs)],val_losses,label = 'Validation Loss', linestyle=\":\")\n",
    "plt.title(f'{model_size} Model with {data_size} data: Epochs')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Looping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "lrs = np.arange(.001,.01,.001)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "start = time.time()\n",
    "\n",
    "test_parameter = 'Learning Rate'\n",
    "\n",
    "#beta_range = np.arange(0,1,.05)\n",
    "#betas = list(product(beta_range,beta_range))\n",
    "train_fin_losses = []\n",
    "val_fin_losses = []\n",
    "i_times = 0\n",
    "for lr in lrs:\n",
    "    start_time_loop = time.time()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)\n",
    "    ## Train and valid loops\n",
    "    for epoch in range(epochs): #edited to slow down\n",
    "        # train\n",
    "        train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "        train_loss, points = 0.0, 0\n",
    "        model.train()\n",
    "        i=0\n",
    "        for x, y in train_loader:\n",
    "            x    = x[:,:,16:-16,16:-16]\n",
    "            bs   = x.shape[0]        #batch size\n",
    "            x    = x.to(device)       #maps\n",
    "            y    = y.to(device)[:,g]  #parameters\n",
    "            i+=1\n",
    "            #print(f'Training:    Epoch={epoch}    Iteration={i}')\n",
    "            p    = model(x)           #NN output\n",
    "            y_NN = p[:,g]             #posterior mean\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            if features==4 or features==12:\n",
    "                e_NN = p[:,h]         #posterior std\n",
    "                loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "                train_loss2 += loss2*bs\n",
    "            else:\n",
    "                loss = torch.mean(torch.log(loss1))\n",
    "            train_loss1 += loss1*bs\n",
    "            points      += bs\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = torch.log(train_loss1/points) \n",
    "        if features==4 or features==12:\n",
    "            train_loss+=torch.log(train_loss2/points)\n",
    "        train_loss = torch.mean(train_loss).item()\n",
    "\n",
    "        i=0\n",
    "\n",
    "        # do validation: cosmo alone & all params\n",
    "        valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "        valid_loss, points = 0.0, 0\n",
    "        model.eval()\n",
    "        for x, y in valid_loader:\n",
    "            with torch.no_grad():\n",
    "                x    = x[:,:,16:-16,16:-16]\n",
    "                bs    = x.shape[0]         #batch size\n",
    "                x     = x.to(device)       #maps\n",
    "                y     = y.to(device)[:,g]  #parameters\n",
    "                i+=1\n",
    "                #print(f'Validation:    Epoch={epoch}    Iteration={i}')\n",
    "                p     = model(x)           #NN output\n",
    "                y_NN  = p[:,g]             #posterior mean\n",
    "                loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "                if features==4 or features==12:    \n",
    "                    e_NN  = p[:,h]         #posterior std\n",
    "                    loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                    valid_loss2 += loss2*bs\n",
    "                valid_loss1 += loss1*bs\n",
    "                points     += bs\n",
    "\n",
    "\n",
    "        valid_loss = torch.log(valid_loss1/points) \n",
    "        if features==4 or features==12:\n",
    "            valid_loss+=torch.log(valid_loss2/points)\n",
    "        valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        # verbose\n",
    "        print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "        print(\"\")\n",
    "    train_fin_losses.append(train_loss)\n",
    "    val_fin_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'{round(i_times/len(lrs),4)*100}% done!')\n",
    "    i_times+=1\n",
    "    \n",
    "    ## Clean GPU\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "    ## Model performance metrics on test set\n",
    "    num_maps=test_loader.dataset.size\n",
    "    ## Now loop over test set and print accuracy\n",
    "    # define the arrays containing the value of the parameters\n",
    "    params_true = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "    params_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "    errors_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "    \n",
    "    end_time_loop = time.time()\n",
    "    time_diff = '(h):', \"{:.4f}\".format((end_time_loop-start_time_loop)/3600.0)\n",
    "    print(f'Loop {i-1} took {time_diff}')\n",
    "    print('Time taken so far in total: (h):', \"{:.4f}\".format((end_time_loop-start)/3600.0))\n",
    "\n",
    "stop = time.time()\n",
    "print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "plt.plot(lrs, train_fin_losses, label = \"Training loss\", linestyle=\"-.\")\n",
    "plt.plot(lrs, val_fin_losses, label = \"Validation Loss\", linestyle=\":\")\n",
    "plt.legend()\n",
    "plt.title(f'{model_size} Model on {data_size} data for {epochs} epochs: {test_parameter}')\n",
    "plt.xlabel(f'{test_parameter}')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#For log_scale learning rate\n",
    "\n",
    "plt.plot(exp, train_fin_losses, label = \"Training loss\", linestyle=\"-.\")\n",
    "plt.plot(exp, val_fin_losses, label = \"Validation Loss\", linestyle=\":\")\n",
    "plt.legend()\n",
    "plt.title(f'{model_size} Model on {data_size} data for {epochs} epochs: {test_parameter}')\n",
    "plt.xlabel('-log(x) (negative of 10^x exponent)')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "X = []\n",
    "Y = []\n",
    "for x,y in betas:\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax.plot(X, Y, val_fin_losses)\n",
    "ax.set_xlabel('Beta1')\n",
    "ax.set_ylabel('Beta2')\n",
    "ax.set_zlabel('Validation Loss')\n",
    "\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax1 = plt.axes(projection='3d')\n",
    "\n",
    "ax1.plot(X, Y, train_fin_losses)\n",
    "ax1.set_xlabel('Beta1')\n",
    "ax1.set_ylabel('Beta2')\n",
    "ax1.set_zlabel('Training Loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(lr,train_fin_losses,label = 'Training Loss', linestyle=\"-.\")\n",
    "plt.plot(lr,val_fin_losses,label = 'Validation Loss', linestyle=\":\")\n",
    "plt.title(f'{model_size} Model with {data_size} data: Learning Rate')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "minpos = val_fin_losses.index(min(val_fin_losses))\n",
    "print('Min Validation is equal to:', val_fin_losses[minpos])\n",
    "print('Which corresponds to a training loss of:', train_fin_losses[minpos])\n",
    "print(f'And is achieved by the {training_parameter} =', lrs[minpos])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "minpos = val_fin_losses.index(min(val_fin_losses))\n",
    "print('Min Validation is equal to:', val_fin_losses[minpos])\n",
    "print('Which corresponds to a training loss of:', train_fin_losses[minpos])\n",
    "print(f'And is achieved by the b1 =',X[minpos], 'and b2 =',Y[minpos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test loss\n",
    "\n",
    "test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "test_loss, points = 0.0, 0\n",
    "model.eval()\n",
    "for x, y in test_loader:\n",
    "    print(points)\n",
    "    with torch.no_grad():\n",
    "        x    = x[:,:,16:-16,16:-16]\n",
    "        bs    = x.shape[0]         #batch size\n",
    "        x     = x.to(device)       #send data to device\n",
    "        y     = y.to(device)[:,g]  #send data to device\n",
    "        p     = model(x)           #prediction for mean and variance\n",
    "        y_NN  = p[:,g]             #prediction for mean\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        if features==4 or features==12:\n",
    "            e_NN  = p[:,h]         #posterior std\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            test_loss2 += loss2*bs\n",
    "        test_loss1 += loss1*bs\n",
    "        test_loss = torch.log(test_loss1/points)\n",
    "        if features==4 or features==12:\n",
    "            test_loss+=torch.log(test_loss2/points)\n",
    "        test_loss = torch.mean(test_loss).item()\n",
    "\n",
    "        # save results to their corresponding arrays\n",
    "        params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "        params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "        if features==4 or features==12:\n",
    "            errors_NN[points:points+x.shape[0]]   = np.abs(e_NN.cpu().numpy())\n",
    "        points    += x.shape[0]\n",
    "test_loss = torch.log(test_loss1/points) + torch.log(test_loss2/points)\n",
    "test_loss = torch.mean(test_loss).item()\n",
    "print('Test loss = %.3e\\n'%test_loss)\n",
    "\n",
    "# de-normalize\n",
    "## I guess these are the hardcoded parameter limits\n",
    "minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "\n",
    "## Drop feedback parameters if they aren't included\n",
    "minimum=minimum[g]\n",
    "maximum=maximum[g]\n",
    "params_true = params_true*(maximum - minimum) + minimum\n",
    "params_NN   = params_NN*(maximum - minimum) + minimum\n",
    "\n",
    "\n",
    "test_error = 100*np.mean(np.sqrt((params_true - params_NN)**2)/params_true,axis=0)\n",
    "print('Error Omega_m = %.3f'%test_error[0])\n",
    "print('Error sigma_8 = %.3f'%test_error[1])\n",
    "\n",
    "wandb.run.summary[\"Error Omega_m\"]=test_error[0]\n",
    "wandb.run.summary[\"Error sigma_8\"]=test_error[1]\n",
    "\n",
    "if features>4:\n",
    "    print('Error A_SN1   = %.3f'%test_error[2])\n",
    "    print('Error A_AGN1  = %.3f'%test_error[3])\n",
    "    print('Error A_SN2   = %.3f'%test_error[4])\n",
    "    print('Error A_AGN2  = %.3f\\n'%test_error[5])\n",
    "    wandb.run.summary[\"Error A_SN1\"]  =test_error[2]\n",
    "    wandb.run.summary[\"Error A_AGN1\"] =test_error[3]\n",
    "    wandb.run.summary[\"Error A_SN2\"]  =test_error[4]\n",
    "    wandb.run.summary[\"Error A_AGN2\"] =test_error[5]\n",
    "\n",
    "wandb.run.summary[\"Error Omega_m\"]=test_error[0]\n",
    "wandb.run.summary[\"Error sigma_8\"]=test_error[1]\n",
    "\n",
    "if features==4:\n",
    "    errors_NN   = errors_NN*(maximum - minimum)\n",
    "    mean_error = 100*(np.absolute(np.mean(errors_NN/params_NN, axis=0)))\n",
    "    print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "    print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "    wandb.run.summary[\"Predicted error Omega_m\"]=mean_error[0]\n",
    "    wandb.run.summary[\"Predicted error sigma_8\"]=mean_error[1]\n",
    "\n",
    "elif features==12:\n",
    "    errors_NN   = errors_NN*(maximum - minimum)\n",
    "    mean_error = 100*(np.absolute(np.mean(errors_NN/params_NN, axis=0)))\n",
    "    print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "    print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "    print('Bayesian error A_SN1   = %.3f'%mean_error[2])\n",
    "    print('Bayesian error A_AGN1  = %.3f'%mean_error[3])\n",
    "    print('Bayesian error A_SN2   = %.3f'%mean_error[4])\n",
    "    print('Bayesian error A_AGN2  = %.3f\\n'%mean_error[5])\n",
    "    wandb.run.summary[\"Predicted error Omega_m\"]=mean_error[0]\n",
    "    wandb.run.summary[\"Predicted error sigma_8\"]=mean_error[1]\n",
    "    wandb.run.summary[\"Predicted error A_SN1\"]  =mean_error[2]\n",
    "    wandb.run.summary[\"Predicted error A_AGN1\"] =mean_error[3]\n",
    "    wandb.run.summary[\"Predicted error A_SN2\"]  =mean_error[4]\n",
    "    wandb.run.summary[\"Predicted error A_AGN2\"] =mean_error[5]\n",
    "\n",
    "\n",
    "if features<5:\n",
    "    f, axarr = plt.subplots(1, 2, figsize=(9,6))\n",
    "    axarr[0].plot(np.linspace(min(params_true[:,0]),max(params_true[:,0]),100),np.linspace(min(params_true[:,0]),max(params_true[:,0]),100),color=\"black\")\n",
    "    axarr[1].plot(np.linspace(min(params_true[:,1]),max(params_true[:,1]),100),np.linspace(min(params_true[:,1]),max(params_true[:,1]),100),color=\"black\")\n",
    "    if features==4:\n",
    "        axarr[0].errorbar(params_true[:,0],params_NN[:,0],errors_NN[:,0],marker=\"o\",ls=\"none\")\n",
    "        axarr[1].errorbar(params_true[:,1],params_NN[:,1],errors_NN[:,1],marker=\"o\",ls=\"none\")\n",
    "    else:\n",
    "        axarr[0].plot(params_true[:,0],params_NN[:,0],marker=\"o\",ls=\"none\")\n",
    "        axarr[1].plot(params_true[:,1],params_NN[:,1],marker=\"o\",ls=\"none\")\n",
    "        \n",
    "    axarr[0].set_xlabel(r\"True $\\Omega_m$\")\n",
    "    axarr[0].set_ylabel(r\"Predicted $\\Omega_m$\")\n",
    "    axarr[0].text(0.1,0.9,\"%.3f %% error\" % test_error[0],fontsize=12,transform=axarr[0].transAxes)\n",
    "\n",
    "    axarr[1].set_xlabel(r\"True $\\sigma_8$\")\n",
    "    axarr[1].set_ylabel(r\"Predicted $\\sigma_8$\")\n",
    "    axarr[1].text(0.1,0.9,\"%.3f %% error\" % test_error[1],fontsize=12,transform=axarr[1].transAxes)\n",
    "\n",
    "\n",
    "if features>4:\n",
    "    f, axarr = plt.subplots(3, 2, figsize=(14,20))\n",
    "    for aa in range(0,6,2):\n",
    "        axarr[aa//2][0].plot(np.linspace(min(params_true[:,aa]),max(params_true[:,aa]),100),np.linspace(min(params_true[:,aa]),max(params_true[:,aa]),100),color=\"black\")\n",
    "        axarr[aa//2][1].plot(np.linspace(min(params_true[:,aa+1]),max(params_true[:,aa+1]),100),np.linspace(min(params_true[:,aa+1]),max(params_true[:,aa+1]),100),color=\"black\")\n",
    "        if features==12:\n",
    "            axarr[aa//2][0].errorbar(params_true[:,aa],params_NN[:,aa],errors_NN[:,aa],marker=\"o\",ls=\"none\")\n",
    "            axarr[aa//2][1].errorbar(params_true[:,aa+1],params_NN[:,aa+1],errors_NN[:,aa+1],marker=\"o\",ls=\"none\")\n",
    "        else:\n",
    "            axarr[aa//2][0].plot(params_true[:,aa],params_NN[:,aa],marker=\"o\",ls=\"none\")\n",
    "            axarr[aa//2][1].plot(params_true[:,aa+1],params_NN[:,aa+1],marker=\"o\",ls=\"none\")\n",
    "            \n",
    "    axarr[0][0].set_xlabel(r\"True $\\Omega_m$\")\n",
    "    axarr[0][0].set_ylabel(r\"Predicted $\\Omega_m$\")\n",
    "    axarr[0][0].text(0.1,0.9,\"%.3f %% error\" % test_error[0],fontsize=12,transform=axarr[0][0].transAxes)\n",
    "\n",
    "    axarr[0][1].set_xlabel(r\"True $\\sigma_8$\")\n",
    "    axarr[0][1].set_ylabel(r\"Predicted $\\sigma_8$\")\n",
    "    axarr[0][1].text(0.1,0.9,\"%.3f %% error\" % test_error[1],fontsize=12,transform=axarr[0][1].transAxes)\n",
    "\n",
    "    axarr[1][0].set_xlabel(r\"True $A_\\mathrm{SN1}$\")\n",
    "    axarr[1][0].set_ylabel(r\"Predicted $A_\\mathrm{SN1}$\")\n",
    "    axarr[1][0].text(0.1,0.9,\"%.3f %% error\" % test_error[2],fontsize=12,transform=axarr[1][0].transAxes)\n",
    "\n",
    "    axarr[1][1].set_xlabel(r\"True $A_\\mathrm{AGN1}$\")\n",
    "    axarr[1][1].set_ylabel(r\"Predicted $A_\\mathrm{AGN1}$\")\n",
    "    axarr[1][1].text(0.1,0.9,\"%.3f %% error\" % test_error[3],fontsize=12,transform=axarr[1][1].transAxes)\n",
    "\n",
    "    axarr[2][0].set_xlabel(r\"True $A_\\mathrm{SN2}$\")\n",
    "    axarr[2][0].set_ylabel(r\"Predicted $A_\\mathrm{SN2}$\")\n",
    "    axarr[2][0].text(0.1,0.9,\"%.3f %% error\" % test_error[4],fontsize=12,transform=axarr[2][0].transAxes)\n",
    "\n",
    "    axarr[2][1].set_xlabel(r\"True $A_\\mathrm{AGN2}$\")\n",
    "    axarr[2][1].set_ylabel(r\"Predicted $A_\\mathrm{AGN2}$\")\n",
    "    axarr[2][1].text(0.1,0.9,\"%.3f %% error\" % test_error[4],fontsize=12,transform=axarr[2][1].transAxes)\n",
    "\n",
    "figure=wandb.Image(f)\n",
    "wandb.log({\"performance\": figure})\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"success_retro.wav\",autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "098e687a0ceb167341ba29473143ae147a995faf0ebe438c7d67b022c8c7c5a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
