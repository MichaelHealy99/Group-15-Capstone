{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example script to run a learnable wavelet scattering network on a subset of the CAMELs data\n",
    "\n",
    "Simple example script. **Make sure you have run the `wget` command in the README to download the example data file**\n",
    "\n",
    "We take a small sample of the CAMELs dataset (1k $M_\\mathrm{tot}$ maps), and train a \"SN\" (2 layers of wavelet convolutions, 8 wavelet filters each, and pass the output to a CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelhealy\u001b[0m (\u001b[33mwaveletcapstone\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install wandb -qqq\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time, sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Learnable scattering modules\n",
    "from learnable_wavelets.models import models_factory, sn_hybrid_models, camels_models\n",
    "from learnable_wavelets.datasets import camels_dataset \n",
    "\n",
    "# my modules\n",
    "from learnable_wavelets.models.models_factory import baseModelFactory, topModelFactory\n",
    "from learnable_wavelets.models.sn_hybrid_models import sn_HybridModel\n",
    "from learnable_wavelets.models.camels_models import get_architecture\n",
    "#from learnable_wavelets.camels.camels_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training parameters\n",
    "batch_size  = 32\n",
    "num_workers = 10    #number of workers to load data\n",
    "lr_sn       = 0.01  ## Learning rate for scattering layers\n",
    "lr          = 0.001 ## Learning rate for neural weights\n",
    "wd          = 0.1   ## weight decay\n",
    "epochs      = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN values\n",
    "# get the value of the hyperparameters\n",
    "lr     = 1e-4\n",
    "wd     = 0.01\n",
    "dr     = 0.1\n",
    "lr_sn  = 0.005\n",
    "hidden=6 ## Number of hidden layers in the top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n"
     ]
    }
   ],
   "source": [
    "# use GPUs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "    use_cuda=True\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "    use_cuda=False\n",
    "cudnn.benchmark = True      #May train faster but cost more memory\n",
    "\n",
    "# architecture parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "fparams    = \"/scratch/cp3759/camels_data/params_IllustrisTNG.txt\"\n",
    "#fparams    = \"../test_data/params_IllustrisTNG.txt\" ## Simulation parameter file\n",
    "fmaps      = [\"/scratch/cp3759/camels_data/Maps_Mtot_IllustrisTNG_LH_z=0.00.npy\"]\n",
    "#fmaps      = [\"../test_data/maps_Mtot_1k.npy\"]   ## Simulated maps, must be a list as we can take multiple maps\n",
    "\n",
    "fmaps_norm      = [None]\n",
    "splits          = 15      ## Number of maps taken from each sim (range between 1 and 15, must match the dataset size)\n",
    "                         ## i.e. splits=1 for 1k maps, splits=15 for 15k maps, or any integer value in between\n",
    "seed            = 123    ## seed for the test/valid/train split\n",
    "monopole        = True   ## Keep the monopole of the maps (True) or remove it (False)\n",
    "rot_flip_in_mem = False  ## Whether rotations and flipings are kept in memory (faster but takes more memory if true)\n",
    "smoothing       = 0      ## Smooth the maps with a Gaussian filter? 0 for no\n",
    "arch            = \"sn\"   ## Which model architecture to use\n",
    "features        = 4     ## Number of variables to train the model on. This can be 2, 4, 6 or 12, depending on whether\n",
    "                         ## you want to a) train on both cosmological and IGM parameters and b) also ask the\n",
    "                         ## network to estimate uncertanties on these parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Load and plot a random map\n",
    "maps = np.load(fmaps[0])\n",
    "maps=maps[np.random.randint(len(maps))]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(np.log10(maps))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=len(fmaps)\n",
    "## Set up indices to use for the loss function\n",
    "if features==2:\n",
    "    g=[0,1]\n",
    "if features==4:\n",
    "    g=[0,1]\n",
    "    h=[2,3]\n",
    "elif features==6:\n",
    "    g=[0,1,2,3,4,5]\n",
    "elif features==12:\n",
    "    g=[0,1,2,3,4,5]\n",
    "    h=[6,7,8,9,10,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelhealy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/mbh425/LearnableWavelets/playground/wandb/run-20221209_173243-wii3j3w5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/michaelhealy/Capstone%20SN%20Hybrid/runs/wii3j3w5\" target=\"_blank\">distinctive-firebrand-5</a></strong> to <a href=\"https://wandb.ai/michaelhealy/Capstone%20SN%20Hybrid\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/michaelhealy/Capstone%20SN%20Hybrid/runs/wii3j3w5?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1542c3e71040>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"learning rate\": lr,\n",
    "            \"scattering learning rate\": lr_sn,\n",
    "            \"wd\": wd,\n",
    "            \"channels\": channels,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch size\": batch_size,\n",
    "            \"network\": arch,\n",
    "            \"features\": features,\n",
    "            \"splits\":splits}\n",
    "\n",
    "wandb.init(project=\"Capstone SN Hybrid\", entity=\"michaelhealy\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing training set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "4.598e+09 < F(all|orig) < 3.186e+15\n",
      "9.663 < F(all|resc)  < 15.503\n",
      "-2.931 < F(all|norm) < 8.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing validation set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "4.598e+09 < F(all|orig) < 3.186e+15\n",
      "9.663 < F(all|resc)  < 15.503\n",
      "-2.931 < F(all|norm) < 8.946\n",
      "\n",
      "Preparing test set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "4.598e+09 < F(all|orig) < 3.186e+15\n",
      "9.663 < F(all|resc)  < 15.503\n",
      "-2.931 < F(all|norm) < 8.946\n"
     ]
    }
   ],
   "source": [
    "## Generate torch datasets\n",
    "print('\\nPreparing training set')\n",
    "train_loader = camels_dataset.create_dataset_multifield('train', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                         num_workers=num_workers, rot_flip_in_mem=rot_flip_in_mem, verbose=True)\n",
    "\n",
    "# get validation set\n",
    "print('\\nPreparing validation set')\n",
    "valid_loader = camels_dataset.create_dataset_multifield('valid', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                         num_workers=num_workers, rot_flip_in_mem=rot_flip_in_mem,  verbose=True)    \n",
    "\n",
    "# get test set\n",
    "print('\\nPreparing test set')\n",
    "test_loader = camels_dataset.create_dataset_multifield('test', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                        num_workers=num_workers, rot_flip_in_mem=rot_flip_in_mem,  verbose=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Create the wavelet scattering layers\n",
    "scatteringBase = models_factory.baseModelFactory(\n",
    "                                architecture='scattering',\n",
    "                                J=2,\n",
    "                                N=256,\n",
    "                                M=256,\n",
    "                                channels=channels,\n",
    "                                max_order=2,\n",
    "                                initialization=\"Random\",\n",
    "                                seed=234,\n",
    "                                learnable=True,\n",
    "                                lr_orientation=lr_sn,\n",
    "                                lr_scattering=lr_sn,\n",
    "                                skip=True,\n",
    "                                split_filters=True,\n",
    "                                subsample=4,\n",
    "                                device=device,\n",
    "                                use_cuda=use_cuda,\n",
    "                                plot=False\n",
    "                                )\n",
    "\n",
    "## Now create a network to follow the scattering layers\n",
    "## can be MLP, linear, or cnn at the moment\n",
    "## (as in https://github.com/bentherien/ParametricScatteringNetworks/ )\n",
    "top = models_factory.topModelFactory(\n",
    "    base=scatteringBase,\n",
    "    architecture=\"cnn\", ## Can be \"cnn\", \"linear_layer\", or \"mlp\"\n",
    "    num_classes=features,\n",
    "    width=3,                    ## Number of hidden layers (only used for cnn and mlp)\n",
    "    average=True,\n",
    "    use_cuda=True\n",
    ")\n",
    "\n",
    "## Merge scattering and neural layers into a single torch module\n",
    "hybridModel = sn_hybrid_models.sn_HybridModel(scatteringBase=scatteringBase, top=top, use_cuda=use_cuda)\n",
    "model=hybridModel\n",
    "model.to(device=device) ## Put model on the appropriate device\n",
    "print(\"Scattering + neural model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_maps=len(train_loader.dataset.x)\n",
    "wandb.config.update({\"no. training maps\": num_train_maps,\n",
    "                        \"fields\": fmaps})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = get_architecture(arch,features,hidden,dr,channels)\n",
    "wandb.config.update({\"learnable_parameters\":sum(p.numel() for p in model.parameters())})\n",
    "model.to(device=device) ## Put model on the appropriate device\n",
    "print(\"sn model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/learnable_wavelets/scattering/create_filters.py:233: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  waves = torch.exp(1.0j * torch.matmul(grid.T, wave_vectors.T).T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scattering + neural model initialised\n"
     ]
    }
   ],
   "source": [
    "## Create the wavelet scattering layers\n",
    "scatteringBase = models_factory.baseModelFactory(\n",
    "                                architecture='scattering',\n",
    "                                J=2,\n",
    "                                N=256,\n",
    "                                M=256,\n",
    "                                channels=channels,\n",
    "                                max_order=2,\n",
    "                                initialization=\"Random\",\n",
    "                                seed=234,\n",
    "                                learnable=True,\n",
    "                                lr_orientation=lr_sn,\n",
    "                                lr_scattering=lr_sn,\n",
    "                                skip=True,\n",
    "                                split_filters=True,\n",
    "                                subsample=4,\n",
    "                                device=device,\n",
    "                                use_cuda=use_cuda,\n",
    "                                plot=False\n",
    "                                )\n",
    "\n",
    "## Now create a network to follow the scattering layers\n",
    "## can be MLP, linear, or cnn at the moment\n",
    "## (as in https://github.com/bentherien/ParametricScatteringNetworks/ )\n",
    "top = models_factory.topModelFactory(\n",
    "    base=scatteringBase,\n",
    "    architecture=\"cnn\", ## Can be \"cnn\", \"linear_layer\", or \"mlp\"\n",
    "    num_classes=features,\n",
    "    width=3,                    ## Number of hidden layers (only used for cnn and mlp)\n",
    "    average=True,\n",
    "    use_cuda=True\n",
    ")\n",
    "\n",
    "## Merge scattering and neural layers into a single torch module\n",
    "hybridModel = sn_hybrid_models.sn_HybridModel(scatteringBase=scatteringBase, top=top, use_cuda=use_cuda)\n",
    "model=hybridModel\n",
    "model.to(device=device) ## Put model on the appropriate device\n",
    "print(\"Scattering + neural model initialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(model, log_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 -8.546e+00 -5.068e+00 \n",
      "001 -1.074e+01 -9.642e+00 \n",
      "002 -1.131e+01 -9.723e+00 \n",
      "003 -1.176e+01 -8.263e+00 \n",
      "004 -1.206e+01 -1.267e+01 \n",
      "005 -1.206e+01 -1.268e+01 \n",
      "006 -1.265e+01 -1.061e+01 \n",
      "007 -1.281e+01 -1.514e+01 \n"
     ]
    }
   ],
   "source": [
    "## Train and valid loops\n",
    "start = time.time()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    log_dic={}\n",
    "    # train\n",
    "    train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for x, y in train_loader:\n",
    "        bs   = x.shape[0]        #batch size\n",
    "        x    = x.to(device)       #maps\n",
    "        y    = y.to(device)[:,g]  #parameters\n",
    "        i+=1\n",
    "        #print(f'Training:    Epoch={epoch}    Iteration={i}')\n",
    "        p    = model(x)           #NN output\n",
    "        y_NN = p[:,g]             #posterior mean\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        if features==4 or features==12:\n",
    "            e_NN = p[:,h]         #posterior std\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "            train_loss2 += loss2*bs\n",
    "        else:\n",
    "            loss = torch.mean(torch.log(loss1))\n",
    "        train_loss1 += loss1*bs\n",
    "        points      += bs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = torch.log(train_loss1/points) \n",
    "    if features==4 or features==12:\n",
    "        train_loss+=torch.log(train_loss2/points)\n",
    "    train_loss = torch.mean(train_loss).item()\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    # do validation: cosmo alone & all params\n",
    "    valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    valid_loss, points = 0.0, 0\n",
    "    model.eval()\n",
    "    for x, y in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            bs    = x.shape[0]         #batch size\n",
    "            x     = x.to(device)       #maps\n",
    "            y     = y.to(device)[:,g]  #parameters\n",
    "            i+=1\n",
    "            #print(f'Validation:    Epoch={epoch}    Iteration={i}')\n",
    "            p     = model(x)           #NN output\n",
    "            y_NN  = p[:,g]             #posterior mean\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            if features==4 or features==12:    \n",
    "                e_NN  = p[:,h]         #posterior std\n",
    "                loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                valid_loss2 += loss2*bs\n",
    "            valid_loss1 += loss1*bs\n",
    "            points     += bs\n",
    "\n",
    "\n",
    "    valid_loss = torch.log(valid_loss1/points) \n",
    "    if features==4 or features==12:\n",
    "        valid_loss+=torch.log(valid_loss2/points)\n",
    "    valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "    log_dic[\"training_loss\"]=train_loss\n",
    "    log_dic[\"valid_loss\"]=valid_loss\n",
    "    wandb.log(log_dic)\n",
    "\n",
    "    # verbose\n",
    "    print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "    print(\"\")\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)\n",
    "\n",
    "stop = time.time()\n",
    "print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))\n",
    "\n",
    "## Model performance metrics on test set\n",
    "num_maps=test_loader.dataset.size\n",
    "## Now loop over test set and print accuracy\n",
    "# define the arrays containing the value of the parameters\n",
    "params_true = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "params_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "errors_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*[range(epochs)],train_losses,label = 'Training Loss', linestyle=\"-.\")\n",
    "plt.plot(*[range(epochs)],val_losses,label = 'Validation Loss', linestyle=\":\")\n",
    "plt.title(f'{model_size} Model with {data_size} data: Epochs')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test loss\n",
    "test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "test_loss, points = 0.0, 0\n",
    "model.eval()\n",
    "for x, y in test_loader:\n",
    "    print(points)\n",
    "    with torch.no_grad():\n",
    "        print(x.shape)\n",
    "        bs    = x.shape[0]         #batch size\n",
    "        x     = x.to(device)       #send data to device\n",
    "        y     = y.to(device)[:,g]  #send data to device\n",
    "        p     = model(x)           #prediction for mean and variance\n",
    "        y_NN  = p[:,g]             #prediction for mean\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        if features==4 or features==12:\n",
    "            e_NN  = p[:,h]         #posterior std\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            test_loss2 += loss2*bs\n",
    "        test_loss1 += loss1*bs\n",
    "        test_loss = torch.log(test_loss1/points)\n",
    "        if features==4 or features==12:\n",
    "            test_loss+=torch.log(test_loss2/points)\n",
    "        test_loss = torch.mean(test_loss).item()\n",
    "\n",
    "        # save results to their corresponding arrays\n",
    "        params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "        params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "        if features==4 or features==12:\n",
    "            errors_NN[points:points+x.shape[0]]   = np.abs(e_NN.cpu().numpy())\n",
    "        points    += x.shape[0]\n",
    "test_loss = torch.log(test_loss1/points) + torch.log(test_loss2/points)\n",
    "test_loss = torch.mean(test_loss).item()\n",
    "print('Test loss = %.3e\\n'%test_loss)\n",
    "\n",
    "# de-normalize\n",
    "## I guess these are the hardcoded parameter limits\n",
    "minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "\n",
    "## Drop feedback parameters if they aren't included\n",
    "minimum=minimum[g]\n",
    "maximum=maximum[g]\n",
    "params_true = params_true*(maximum - minimum) + minimum\n",
    "params_NN   = params_NN*(maximum - minimum) + minimum\n",
    "\n",
    "\n",
    "test_error = 100*np.mean(np.sqrt((params_true - params_NN)**2)/params_true,axis=0)\n",
    "print('Error Omega_m = %.3f'%test_error[0])\n",
    "print('Error sigma_8 = %.3f'%test_error[1])\n",
    "\n",
    "wandb.run.summary[\"Error Omega_m\"]=test_error[0]\n",
    "wandb.run.summary[\"Error sigma_8\"]=test_error[1]\n",
    "\n",
    "if features>4:\n",
    "    print('Error A_SN1   = %.3f'%test_error[2])\n",
    "    print('Error A_AGN1  = %.3f'%test_error[3])\n",
    "    print('Error A_SN2   = %.3f'%test_error[4])\n",
    "    print('Error A_AGN2  = %.3f\\n'%test_error[5])\n",
    "    wandb.run.summary[\"Error A_SN1\"]  =test_error[2]\n",
    "    wandb.run.summary[\"Error A_AGN1\"] =test_error[3]\n",
    "    wandb.run.summary[\"Error A_SN2\"]  =test_error[4]\n",
    "    wandb.run.summary[\"Error A_AGN2\"] =test_error[5]\n",
    "\n",
    "wandb.run.summary[\"Error Omega_m\"]=test_error[0]\n",
    "wandb.run.summary[\"Error sigma_8\"]=test_error[1]\n",
    "\n",
    "if features==4:\n",
    "    errors_NN   = errors_NN*(maximum - minimum)\n",
    "    mean_error = 100*(np.absolute(np.mean(errors_NN/params_NN, axis=0)))\n",
    "    print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "    print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "    wandb.run.summary[\"Predicted error Omega_m\"]=mean_error[0]\n",
    "    wandb.run.summary[\"Predicted error sigma_8\"]=mean_error[1]\n",
    "\n",
    "elif features==12:\n",
    "    errors_NN   = errors_NN*(maximum - minimum)\n",
    "    mean_error = 100*(np.absolute(np.mean(errors_NN/params_NN, axis=0)))\n",
    "    print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "    print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "    print('Bayesian error A_SN1   = %.3f'%mean_error[2])\n",
    "    print('Bayesian error A_AGN1  = %.3f'%mean_error[3])\n",
    "    print('Bayesian error A_SN2   = %.3f'%mean_error[4])\n",
    "    print('Bayesian error A_AGN2  = %.3f\\n'%mean_error[5])\n",
    "    wandb.run.summary[\"Predicted error Omega_m\"]=mean_error[0]\n",
    "    wandb.run.summary[\"Predicted error sigma_8\"]=mean_error[1]\n",
    "    wandb.run.summary[\"Predicted error A_SN1\"]  =mean_error[2]\n",
    "    wandb.run.summary[\"Predicted error A_AGN1\"] =mean_error[3]\n",
    "    wandb.run.summary[\"Predicted error A_SN2\"]  =mean_error[4]\n",
    "    wandb.run.summary[\"Predicted error A_AGN2\"] =mean_error[5]\n",
    "\n",
    "\n",
    "if features<5:\n",
    "    f, axarr = plt.subplots(1, 2, figsize=(9,6))\n",
    "    axarr[0].plot(np.linspace(min(params_true[:,0]),max(params_true[:,0]),100),np.linspace(min(params_true[:,0]),max(params_true[:,0]),100),color=\"black\")\n",
    "    axarr[1].plot(np.linspace(min(params_true[:,1]),max(params_true[:,1]),100),np.linspace(min(params_true[:,1]),max(params_true[:,1]),100),color=\"black\")\n",
    "    if features==4:\n",
    "        axarr[0].errorbar(params_true[:,0],params_NN[:,0],errors_NN[:,0],marker=\"o\",ls=\"none\")\n",
    "        axarr[1].errorbar(params_true[:,1],params_NN[:,1],errors_NN[:,1],marker=\"o\",ls=\"none\")\n",
    "    else:\n",
    "        axarr[0].plot(params_true[:,0],params_NN[:,0],marker=\"o\",ls=\"none\")\n",
    "        axarr[1].plot(params_true[:,1],params_NN[:,1],marker=\"o\",ls=\"none\")\n",
    "        \n",
    "    axarr[0].set_xlabel(r\"True $\\Omega_m$\")\n",
    "    axarr[0].set_ylabel(r\"Predicted $\\Omega_m$\")\n",
    "    axarr[0].text(0.1,0.9,\"%.3f %% error\" % test_error[0],fontsize=12,transform=axarr[0].transAxes)\n",
    "\n",
    "    axarr[1].set_xlabel(r\"True $\\sigma_8$\")\n",
    "    axarr[1].set_ylabel(r\"Predicted $\\sigma_8$\")\n",
    "    axarr[1].text(0.1,0.9,\"%.3f %% error\" % test_error[1],fontsize=12,transform=axarr[1].transAxes)\n",
    "\n",
    "\n",
    "if features>4:\n",
    "    f, axarr = plt.subplots(3, 2, figsize=(14,20))\n",
    "    for aa in range(0,6,2):\n",
    "        axarr[aa//2][0].plot(np.linspace(min(params_true[:,aa]),max(params_true[:,aa]),100),np.linspace(min(params_true[:,aa]),max(params_true[:,aa]),100),color=\"black\")\n",
    "        axarr[aa//2][1].plot(np.linspace(min(params_true[:,aa+1]),max(params_true[:,aa+1]),100),np.linspace(min(params_true[:,aa+1]),max(params_true[:,aa+1]),100),color=\"black\")\n",
    "        if features==12:\n",
    "            axarr[aa//2][0].errorbar(params_true[:,aa],params_NN[:,aa],errors_NN[:,aa],marker=\"o\",ls=\"none\")\n",
    "            axarr[aa//2][1].errorbar(params_true[:,aa+1],params_NN[:,aa+1],errors_NN[:,aa+1],marker=\"o\",ls=\"none\")\n",
    "        else:\n",
    "            axarr[aa//2][0].plot(params_true[:,aa],params_NN[:,aa],marker=\"o\",ls=\"none\")\n",
    "            axarr[aa//2][1].plot(params_true[:,aa+1],params_NN[:,aa+1],marker=\"o\",ls=\"none\")\n",
    "            \n",
    "    axarr[0][0].set_xlabel(r\"True $\\Omega_m$\")\n",
    "    axarr[0][0].set_ylabel(r\"Predicted $\\Omega_m$\")\n",
    "    axarr[0][0].text(0.1,0.9,\"%.3f %% error\" % test_error[0],fontsize=12,transform=axarr[0][0].transAxes)\n",
    "\n",
    "    axarr[0][1].set_xlabel(r\"True $\\sigma_8$\")\n",
    "    axarr[0][1].set_ylabel(r\"Predicted $\\sigma_8$\")\n",
    "    axarr[0][1].text(0.1,0.9,\"%.3f %% error\" % test_error[1],fontsize=12,transform=axarr[0][1].transAxes)\n",
    "\n",
    "    axarr[1][0].set_xlabel(r\"True $A_\\mathrm{SN1}$\")\n",
    "    axarr[1][0].set_ylabel(r\"Predicted $A_\\mathrm{SN1}$\")\n",
    "    axarr[1][0].text(0.1,0.9,\"%.3f %% error\" % test_error[2],fontsize=12,transform=axarr[1][0].transAxes)\n",
    "\n",
    "    axarr[1][1].set_xlabel(r\"True $A_\\mathrm{AGN1}$\")\n",
    "    axarr[1][1].set_ylabel(r\"Predicted $A_\\mathrm{AGN1}$\")\n",
    "    axarr[1][1].text(0.1,0.9,\"%.3f %% error\" % test_error[3],fontsize=12,transform=axarr[1][1].transAxes)\n",
    "\n",
    "    axarr[2][0].set_xlabel(r\"True $A_\\mathrm{SN2}$\")\n",
    "    axarr[2][0].set_ylabel(r\"Predicted $A_\\mathrm{SN2}$\")\n",
    "    axarr[2][0].text(0.1,0.9,\"%.3f %% error\" % test_error[4],fontsize=12,transform=axarr[2][0].transAxes)\n",
    "\n",
    "    axarr[2][1].set_xlabel(r\"True $A_\\mathrm{AGN2}$\")\n",
    "    axarr[2][1].set_ylabel(r\"Predicted $A_\\mathrm{AGN2}$\")\n",
    "    axarr[2][1].text(0.1,0.9,\"%.3f %% error\" % test_error[4],fontsize=12,transform=axarr[2][1].transAxes)\n",
    "\n",
    "figure=wandb.Image(f)\n",
    "wandb.log({\"performance\": figure})\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"success_retro.wav\",autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env2",
   "language": "python",
   "name": "my_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
