{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example script to run a learnable wavelet scattering network on a subset of the CAMELs data\n",
    "\n",
    "Simple example script. **Make sure you have run the `wget` command in the README to download the example data file**\n",
    "\n",
    "We take a small sample of the CAMELs dataset (1k $M_\\mathrm{tot}$ maps), and train a \"SN\" (2 layers of wavelet convolutions, 8 wavelet filters each, and pass the output to a CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelhealy\u001b[0m (\u001b[33mwaveletcapstone\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install wandb -qqq\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Learnable scattering modules\n",
    "from learnable_wavelets.models import models_factory, sn_hybrid_models, camels_models\n",
    "from learnable_wavelets.datasets import camels_dataset \n",
    "\n",
    "## MaxViT\n",
    "import timm\n",
    "\n",
    "from itertools import product\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training parameters\n",
    "batch_size  = 32\n",
    "num_workers = 10    #number of workers to load data\n",
    "lr_sn       = 0.01  ## Learning rate for scattering layers\n",
    "lr          = 0.005 ## Learning rate for neural weights\n",
    "wd          = 0.18   ## weight decay\n",
    "epochs      = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n"
     ]
    }
   ],
   "source": [
    "# use GPUs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "    use_cuda=True\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "    use_cuda=False\n",
    "cudnn.benchmark = True      #May train faster but cost more memory\n",
    "\n",
    "# architecture parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "fparams    = \"/scratch/cp3759/camels_data/params_IllustrisTNG.txt\"\n",
    "#fparams    = \"../test_data/params_IllustrisTNG.txt\" ## Simulation parameter file\n",
    "fmaps      = [\"/scratch/cp3759/camels_data/Maps_Mtot_IllustrisTNG_LH_z=0.00.npy\"]\n",
    "#fmaps      = [\"../test_data/maps_Mtot_1k.npy\"]   ## Simulated maps, must be a list as we can take multiple maps\n",
    "\n",
    "fmaps_norm      = [None]\n",
    "splits          = 15      ## Number of maps taken from each sim (range between 1 and 15, must match the dataset size)\n",
    "                         ## i.e. splits=1 for 1k maps, splits=15 for 15k maps, or any integer value in between\n",
    "seed            = 123    ## seed for the test/valid/train split\n",
    "monopole        = True   ## Keep the monopole of the maps (True) or remove it (False)\n",
    "rot_flip_in_mem = False  ## Whether rotations and flipings are kept in memory (faster but takes more memory if true)\n",
    "smoothing       = 0      ## Smooth the maps with a Gaussian filter? 0 for no\n",
    "arch            = \"maxvit\"   ## Which model architecture to use\n",
    "features        = 4    ## Number of variables to train the model on. This can be 2, 4, 6 or 12, depending on whether\n",
    "                         ## you want to a) train on both cosmological and IGM parameters and b) also ask the\n",
    "                         ## network to estimate uncertanties on these parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Load and plot a random map\n",
    "maps = np.load(fmaps[0])\n",
    "maps=maps[np.random.randint(len(maps))]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(np.log10(maps))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=len(fmaps)\n",
    "## Set up indices to use for the loss function\n",
    "if features==2:\n",
    "    g=[0,1]\n",
    "if features==4:\n",
    "    g=[0,1]\n",
    "    h=[2,3]\n",
    "elif features==6:\n",
    "    g=[0,1,2,3,4,5]\n",
    "elif features==12:\n",
    "    g=[0,1,2,3,4,5]\n",
    "    h=[6,7,8,9,10,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelhealy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/mbh425/LearnableWavelets/playground/wandb/run-20221209_163539-gb7q34kw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/michaelhealy/Capstone%20Maxvit%20Base%2015k%20With%20Transform/runs/gb7q34kw\" target=\"_blank\">mild-mountain-1</a></strong> to <a href=\"https://wandb.ai/michaelhealy/Capstone%20Maxvit%20Base%2015k%20With%20Transform\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/michaelhealy/Capstone%20Maxvit%20Base%2015k%20With%20Transform/runs/gb7q34kw?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x145a18567f10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"learning rate\": lr,\n",
    "            \"scattering learning rate\": lr_sn,\n",
    "            \"wd\": wd,\n",
    "            \"channels\": channels,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch size\": batch_size,\n",
    "            \"network\": arch,\n",
    "            \"features\": features,\n",
    "            \"splits\":splits}\n",
    "\n",
    "wandb.init(project=\"Capstone Maxvit Base 15k With Transform\", entity=\"michaelhealy\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing training set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "4.598e+09 < F(all|orig) < 3.186e+15\n",
      "9.663 < F(all|resc)  < 15.503\n",
      "-2.931 < F(all|norm) < 8.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing validation set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "4.598e+09 < F(all|orig) < 3.186e+15\n",
      "9.663 < F(all|resc)  < 15.503\n",
      "-2.931 < F(all|norm) < 8.946\n",
      "\n",
      "Preparing test set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "4.598e+09 < F(all|orig) < 3.186e+15\n",
      "9.663 < F(all|resc)  < 15.503\n",
      "-2.931 < F(all|norm) < 8.946\n"
     ]
    }
   ],
   "source": [
    "## Generate torch datasets\n",
    "print('\\nPreparing training set')\n",
    "train_loader = camels_dataset.create_dataset_multifield('train', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                         num_workers=num_workers, rot_flip_in_mem=rot_flip_in_mem, verbose=True)\n",
    "\n",
    "# get validation set\n",
    "print('\\nPreparing validation set')\n",
    "valid_loader = camels_dataset.create_dataset_multifield('valid', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                         num_workers=num_workers, rot_flip_in_mem=rot_flip_in_mem,  verbose=True)    \n",
    "\n",
    "# get test set\n",
    "print('\\nPreparing test set')\n",
    "test_loader = camels_dataset.create_dataset_multifield('test', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                        num_workers=num_workers, rot_flip_in_mem=rot_flip_in_mem,  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_maps=len(train_loader.dataset.x)\n",
    "wandb.config.update({\"no. training maps\": num_train_maps,\n",
    "                        \"fields\": fmaps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxViT Base model created\n"
     ]
    }
   ],
   "source": [
    "#Create model\n",
    "model_size = 'Base'\n",
    "data_size = '15k'\n",
    "model = timm.models.maxvit_base_224(in_chans=channels,num_classes=features)\n",
    "#model = timm.models.maxvit_xlarge_224(in_chans=channels,num_classes=features)\n",
    "#model = timm.models.maxxvit_rmlp_tiny_rw_256(in_chans=channels,num_classes=features)\n",
    "wandb.config.update({\"learnable_parameters\":sum(p.numel() for p in model.parameters())})\n",
    "model.to(device=device) ## Put model on the appropriate device\n",
    "print(f\"MaxViT {model_size} model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below counts trainable parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Number of trainable parameters in tiny =',count_parameters(timm.models.maxvit_tiny_224(in_chans=channels,num_classes=features)))\n",
    "print('Number of trainable parameters in small =',count_parameters(timm.models.maxvit_small_224(in_chans=channels,num_classes=features)))\n",
    "print('Number of trainable parameters in base =',count_parameters(timm.models.maxvit_base_224(in_chans=channels,num_classes=features)))\n",
    "print('Number of trainable parameters in large =',count_parameters(timm.models.maxvit_large_224(in_chans=channels,num_classes=features)))\n",
    "print('Number of trainable parameters in their edited maxxvit tiny =',count_parameters(timm.models.maxxvit.maxvit_rmlp_tiny_rw_256(in_chans=channels,num_classes=features)))\n",
    "print('Number of trainable parameters in their edited maxxvit small =',count_parameters(timm.models.maxxvit.maxvit_rmlp_small_rw_256(in_chans=channels,num_classes=features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Model Loop Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log_freq=1)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 15.78 GiB total capacity; 14.35 GiB already allocated; 10.19 MiB free; 14.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#print(f'Training:    Epoch={epoch}    Iteration={i}')\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m p    \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m#NN output\u001b[39;00m\n\u001b[1;32m     20\u001b[0m y_NN \u001b[38;5;241m=\u001b[39m p[:,g]             \u001b[38;5;66;03m#posterior mean\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((y_NN \u001b[38;5;241m-\u001b[39m y)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,                axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/timm/models/maxxvit.py:1700\u001b[0m, in \u001b[0;36mMaxxVit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m-> 1700\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1701\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/timm/models/maxxvit.py:1692\u001b[0m, in \u001b[0;36mMaxxVit.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m   1691\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[0;32m-> 1692\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1693\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/timm/models/maxxvit.py:1537\u001b[0m, in \u001b[0;36mMaxxVitStage.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/timm/models/maxxvit.py:1428\u001b[0m, in \u001b[0;36mMaxxVitBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1426\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# to NHWC (channels-last)\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_block(x)\n\u001b[0;32m-> 1428\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnchw_attn:\n\u001b[1;32m   1430\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# back to NCHW\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/timm/models/maxxvit.py:1215\u001b[0m, in \u001b[0;36mPartitionAttentionCl.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m-> 1215\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partition_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m   1216\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/timm/models/maxxvit.py:1206\u001b[0m, in \u001b[0;36mPartitionAttentionCl._partition_attn\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1204\u001b[0m     partitioned \u001b[38;5;241m=\u001b[39m grid_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition_size)\n\u001b[0;32m-> 1206\u001b[0m partitioned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartitioned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition_block:\n\u001b[1;32m   1209\u001b[0m     x \u001b[38;5;241m=\u001b[39m window_reverse(partitioned, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition_size, img_size)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/timm/models/maxxvit.py:749\u001b[0m, in \u001b[0;36mAttentionCl.forward\u001b[0;34m(self, x, shared_rel_pos)\u001b[0m\n\u001b[1;32m    746\u001b[0m B \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    747\u001b[0m restore_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 749\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_head \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    751\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 15.78 GiB total capacity; 14.35 GiB already allocated; 10.19 MiB free; 14.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "## Train and valid loops\n",
    "start = time.time()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs): #edited to slow down\n",
    "    log_dic={}\n",
    "    # train\n",
    "    train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    i=0\n",
    "    for x, y in train_loader:\n",
    "        x    = x[:,:,16:-16,16:-16]\n",
    "        bs   = x.shape[0]        #batch size\n",
    "        x    = x.to(device)       #maps\n",
    "        y    = y.to(device)[:,g]  #parameters\n",
    "        i+=1\n",
    "        #print(f'Training:    Epoch={epoch}    Iteration={i}')\n",
    "        p    = model(x)           #NN output\n",
    "        y_NN = p[:,g]             #posterior mean\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        if features==4 or features==12:\n",
    "            e_NN = p[:,h]         #posterior std\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "            train_loss2 += loss2*bs\n",
    "        else:\n",
    "            loss = torch.mean(torch.log(loss1))\n",
    "        train_loss1 += loss1*bs\n",
    "        points      += bs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = torch.log(train_loss1/points) \n",
    "    if features==4 or features==12:\n",
    "        train_loss+=torch.log(train_loss2/points)\n",
    "    train_loss = torch.mean(train_loss).item()\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    # do validation: cosmo alone & all params\n",
    "    valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    valid_loss, points = 0.0, 0\n",
    "    model.eval()\n",
    "    for x, y in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            x    = x[:,:,16:-16,16:-16]\n",
    "            x = torch.cat((x,x.flip(3)))\n",
    "            y = torch.cat((y,y))\n",
    "            bs    = x.shape[0]         #batch size\n",
    "            x     = x.to(device)       #maps\n",
    "            y     = y.to(device)[:,g]  #parameters\n",
    "            i+=1\n",
    "            #print(f'Validation:    Epoch={epoch}    Iteration={i}')\n",
    "            p     = model(x)           #NN output\n",
    "            y_NN  = p[:,g]             #posterior mean\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            if features==4 or features==12:    \n",
    "                e_NN  = p[:,h]         #posterior std\n",
    "                loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                valid_loss2 += loss2*bs\n",
    "            valid_loss1 += loss1*bs\n",
    "            points     += bs\n",
    "\n",
    "\n",
    "    valid_loss = torch.log(valid_loss1/points) \n",
    "    if features==4 or features==12:\n",
    "        valid_loss+=torch.log(valid_loss2/points)\n",
    "    valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "    log_dic[\"training_loss\"]=train_loss\n",
    "    log_dic[\"valid_loss\"]=valid_loss\n",
    "    wandb.log(log_dic)\n",
    "\n",
    "    # verbose\n",
    "    print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "    print(\"\")\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)\n",
    "\n",
    "stop = time.time()\n",
    "print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))\n",
    "\n",
    "## Model performance metrics on test set\n",
    "num_maps=test_loader.dataset.size\n",
    "## Now loop over test set and print accuracy\n",
    "# define the arrays containing the value of the parameters\n",
    "params_true = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "params_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "errors_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3374, -0.3721, -0.4578,  ...,  0.4679,  0.4256,  0.5475],\n",
       "          [-0.3277, -0.3488, -0.4178,  ...,  0.1402, -0.0104,  0.0495],\n",
       "          [-0.2833, -0.3978, -0.4694,  ..., -0.0963, -0.1806, -0.2189],\n",
       "          ...,\n",
       "          [ 0.2521,  0.3420,  0.4284,  ...,  0.3993,  0.5832,  0.7669],\n",
       "          [ 0.3780,  0.4490,  0.5075,  ...,  1.1881,  1.4582,  1.8287],\n",
       "          [ 0.5650,  0.6055,  0.6545,  ...,  2.7351,  3.3737,  2.9904]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3346,  0.4174,  0.4182,  ...,  0.2565,  0.1676,  0.7016],\n",
       "          [ 0.1852,  0.3971,  0.5053,  ..., -0.0156,  0.0219,  0.2959],\n",
       "          [ 0.1229,  0.3446,  0.4948,  ..., -0.0509,  0.0646,  0.0090],\n",
       "          ...,\n",
       "          [ 0.5662,  0.5961,  0.6921,  ...,  0.6172,  0.6550,  0.5550],\n",
       "          [ 0.7911,  0.9035,  0.9196,  ...,  0.6272,  0.7788,  0.6418],\n",
       "          [ 0.8559,  0.9488,  0.9067,  ...,  0.6228,  0.8599,  0.8876]]],\n",
       "\n",
       "\n",
       "        [[[-0.5225, -0.6725, -0.7858,  ...,  0.8485,  1.4605,  0.9476],\n",
       "          [-0.5509, -0.6857, -0.7928,  ...,  0.7373,  1.4987,  1.2489],\n",
       "          [-0.5111, -0.6763, -0.7667,  ...,  0.6281,  0.9185,  0.8819],\n",
       "          ...,\n",
       "          [-0.1073,  0.0175, -0.2799,  ..., -0.8576, -0.8754, -0.8868],\n",
       "          [-0.1036, -0.1802, -0.4031,  ..., -0.7797, -0.7210, -0.7071],\n",
       "          [-0.0974, -0.3001, -0.4980,  ..., -0.7420, -0.6375, -0.5856]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.4401, -0.1406,  0.0977,  ..., -0.9858, -0.9541, -0.7950],\n",
       "          [-0.4450, -0.1385,  0.1411,  ..., -0.9966, -0.9820, -0.8746],\n",
       "          [-0.4787, -0.1235,  0.1681,  ..., -0.9932, -0.9546, -0.8987],\n",
       "          ...,\n",
       "          [-0.0994,  0.0547,  0.1500,  ..., -0.4410, -0.4713, -0.3945],\n",
       "          [-0.1425,  0.0046,  0.0572,  ..., -0.4877, -0.4928, -0.4112],\n",
       "          [-0.1435, -0.0432, -0.0281,  ..., -0.4434, -0.4715, -0.3802]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5385,  0.8260,  0.9447,  ..., -0.4171, -0.4657, -0.5563],\n",
       "          [ 0.6485,  0.7112,  0.7672,  ..., -0.3107, -0.3592, -0.4293],\n",
       "          [ 0.6838,  0.6861,  0.7879,  ..., -0.3023, -0.3130, -0.2996],\n",
       "          ...,\n",
       "          [ 1.2697,  1.2984,  1.2106,  ...,  3.4200,  1.4548,  0.7395],\n",
       "          [ 1.8134,  1.7693,  1.3798,  ...,  1.5365,  0.9817,  0.6895],\n",
       "          [ 2.3975,  2.1823,  1.7764,  ...,  0.3860,  0.2915,  0.2656]]],\n",
       "\n",
       "\n",
       "        [[[-0.1766, -0.5914, -0.6835,  ..., -0.8042, -0.7965, -0.7981],\n",
       "          [ 0.0768, -0.5416, -0.7363,  ..., -0.8084, -0.7928, -0.7941],\n",
       "          [ 0.8453, -0.2200, -0.6204,  ..., -0.8177, -0.8081, -0.7914],\n",
       "          ...,\n",
       "          [-0.4714, -0.4097, -0.2716,  ...,  1.9148,  1.2704,  0.7920],\n",
       "          [-0.5976, -0.6254, -0.6534,  ...,  1.4526,  1.1016,  0.6669],\n",
       "          [-0.6503, -0.6735, -0.6983,  ...,  1.7218,  1.0544,  0.8565]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flip(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9955, 0.9125],\n",
       "        [0.8825, 0.9225],\n",
       "        [0.3335, 0.6895],\n",
       "        [0.9495, 0.8235],\n",
       "        [0.2495, 0.2235],\n",
       "        [0.9625, 0.1065],\n",
       "        [0.4485, 0.8425],\n",
       "        [0.8185, 0.8905],\n",
       "        [0.4975, 0.8775],\n",
       "        [0.3215, 0.0145],\n",
       "        [0.8675, 0.0385],\n",
       "        [0.7075, 0.0985],\n",
       "        [0.7325, 0.6325],\n",
       "        [0.1755, 0.2465],\n",
       "        [0.9955, 0.9125],\n",
       "        [0.8825, 0.9225],\n",
       "        [0.3335, 0.6895],\n",
       "        [0.9495, 0.8235],\n",
       "        [0.2495, 0.2235],\n",
       "        [0.9625, 0.1065],\n",
       "        [0.4485, 0.8425],\n",
       "        [0.8185, 0.8905],\n",
       "        [0.4975, 0.8775],\n",
       "        [0.3215, 0.0145],\n",
       "        [0.8675, 0.0385],\n",
       "        [0.7075, 0.0985],\n",
       "        [0.7325, 0.6325],\n",
       "        [0.1755, 0.2465]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((y,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5475,  0.4256,  0.4679,  ..., -0.4578, -0.3721, -0.3374],\n",
       "          [ 0.0495, -0.0104,  0.1402,  ..., -0.4178, -0.3488, -0.3277],\n",
       "          [-0.2189, -0.1806, -0.0963,  ..., -0.4694, -0.3978, -0.2833],\n",
       "          ...,\n",
       "          [ 0.7669,  0.5832,  0.3993,  ...,  0.4284,  0.3420,  0.2521],\n",
       "          [ 1.8287,  1.4582,  1.1881,  ...,  0.5075,  0.4490,  0.3780],\n",
       "          [ 2.9904,  3.3737,  2.7351,  ...,  0.6545,  0.6055,  0.5650]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7016,  0.1676,  0.2565,  ...,  0.4182,  0.4174,  0.3346],\n",
       "          [ 0.2959,  0.0219, -0.0156,  ...,  0.5053,  0.3971,  0.1852],\n",
       "          [ 0.0090,  0.0646, -0.0509,  ...,  0.4948,  0.3446,  0.1229],\n",
       "          ...,\n",
       "          [ 0.5550,  0.6550,  0.6172,  ...,  0.6921,  0.5961,  0.5662],\n",
       "          [ 0.6418,  0.7788,  0.6272,  ...,  0.9196,  0.9035,  0.7911],\n",
       "          [ 0.8876,  0.8599,  0.6228,  ...,  0.9067,  0.9488,  0.8559]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9476,  1.4605,  0.8485,  ..., -0.7858, -0.6725, -0.5225],\n",
       "          [ 1.2489,  1.4987,  0.7373,  ..., -0.7928, -0.6857, -0.5509],\n",
       "          [ 0.8819,  0.9185,  0.6281,  ..., -0.7667, -0.6763, -0.5111],\n",
       "          ...,\n",
       "          [-0.8868, -0.8754, -0.8576,  ..., -0.2799,  0.0175, -0.1073],\n",
       "          [-0.7071, -0.7210, -0.7797,  ..., -0.4031, -0.1802, -0.1036],\n",
       "          [-0.5856, -0.6375, -0.7420,  ..., -0.4980, -0.3001, -0.0974]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.4401, -0.1406,  0.0977,  ..., -0.9858, -0.9541, -0.7950],\n",
       "          [-0.4450, -0.1385,  0.1411,  ..., -0.9966, -0.9820, -0.8746],\n",
       "          [-0.4787, -0.1235,  0.1681,  ..., -0.9932, -0.9546, -0.8987],\n",
       "          ...,\n",
       "          [-0.0994,  0.0547,  0.1500,  ..., -0.4410, -0.4713, -0.3945],\n",
       "          [-0.1425,  0.0046,  0.0572,  ..., -0.4877, -0.4928, -0.4112],\n",
       "          [-0.1435, -0.0432, -0.0281,  ..., -0.4434, -0.4715, -0.3802]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5385,  0.8260,  0.9447,  ..., -0.4171, -0.4657, -0.5563],\n",
       "          [ 0.6485,  0.7112,  0.7672,  ..., -0.3107, -0.3592, -0.4293],\n",
       "          [ 0.6838,  0.6861,  0.7879,  ..., -0.3023, -0.3130, -0.2996],\n",
       "          ...,\n",
       "          [ 1.2697,  1.2984,  1.2106,  ...,  3.4200,  1.4548,  0.7395],\n",
       "          [ 1.8134,  1.7693,  1.3798,  ...,  1.5365,  0.9817,  0.6895],\n",
       "          [ 2.3975,  2.1823,  1.7764,  ...,  0.3860,  0.2915,  0.2656]]],\n",
       "\n",
       "\n",
       "        [[[-0.1766, -0.5914, -0.6835,  ..., -0.8042, -0.7965, -0.7981],\n",
       "          [ 0.0768, -0.5416, -0.7363,  ..., -0.8084, -0.7928, -0.7941],\n",
       "          [ 0.8453, -0.2200, -0.6204,  ..., -0.8177, -0.8081, -0.7914],\n",
       "          ...,\n",
       "          [-0.4714, -0.4097, -0.2716,  ...,  1.9148,  1.2704,  0.7920],\n",
       "          [-0.5976, -0.6254, -0.6534,  ...,  1.4526,  1.1016,  0.6669],\n",
       "          [-0.6503, -0.6735, -0.6983,  ...,  1.7218,  1.0544,  0.8565]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x,x.flip(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*[range(epochs)],train_losses,label = 'Training Loss', linestyle=\"-.\")\n",
    "plt.plot(*[range(epochs)],val_losses,label = 'Validation Loss', linestyle=\":\")\n",
    "plt.title(f'{model_size} Model with {data_size} data: Epochs')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Looping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "lrs = np.arange(.001,.01,.001)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "start = time.time()\n",
    "\n",
    "test_parameter = 'Learning Rate'\n",
    "\n",
    "#beta_range = np.arange(0,1,.05)\n",
    "#betas = list(product(beta_range,beta_range))\n",
    "train_fin_losses = []\n",
    "val_fin_losses = []\n",
    "i_times = 0\n",
    "for lr in lrs:\n",
    "    start_time_loop = time.time()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)\n",
    "    ## Train and valid loops\n",
    "    for epoch in range(epochs): #edited to slow down\n",
    "        # train\n",
    "        train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "        train_loss, points = 0.0, 0\n",
    "        model.train()\n",
    "        i=0\n",
    "        for x, y in train_loader:\n",
    "            x    = x[:,:,16:-16,16:-16]\n",
    "            bs   = x.shape[0]        #batch size\n",
    "            x    = x.to(device)       #maps\n",
    "            y    = y.to(device)[:,g]  #parameters\n",
    "            i+=1\n",
    "            #print(f'Training:    Epoch={epoch}    Iteration={i}')\n",
    "            p    = model(x)           #NN output\n",
    "            y_NN = p[:,g]             #posterior mean\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            if features==4 or features==12:\n",
    "                e_NN = p[:,h]         #posterior std\n",
    "                loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "                train_loss2 += loss2*bs\n",
    "            else:\n",
    "                loss = torch.mean(torch.log(loss1))\n",
    "            train_loss1 += loss1*bs\n",
    "            points      += bs\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = torch.log(train_loss1/points) \n",
    "        if features==4 or features==12:\n",
    "            train_loss+=torch.log(train_loss2/points)\n",
    "        train_loss = torch.mean(train_loss).item()\n",
    "\n",
    "        i=0\n",
    "\n",
    "        # do validation: cosmo alone & all params\n",
    "        valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "        valid_loss, points = 0.0, 0\n",
    "        model.eval()\n",
    "        for x, y in valid_loader:\n",
    "            with torch.no_grad():\n",
    "                x    = x[:,:,16:-16,16:-16]\n",
    "                bs    = x.shape[0]         #batch size\n",
    "                x     = x.to(device)       #maps\n",
    "                y     = y.to(device)[:,g]  #parameters\n",
    "                i+=1\n",
    "                #print(f'Validation:    Epoch={epoch}    Iteration={i}')\n",
    "                p     = model(x)           #NN output\n",
    "                y_NN  = p[:,g]             #posterior mean\n",
    "                loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "                if features==4 or features==12:    \n",
    "                    e_NN  = p[:,h]         #posterior std\n",
    "                    loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                    valid_loss2 += loss2*bs\n",
    "                valid_loss1 += loss1*bs\n",
    "                points     += bs\n",
    "\n",
    "\n",
    "        valid_loss = torch.log(valid_loss1/points) \n",
    "        if features==4 or features==12:\n",
    "            valid_loss+=torch.log(valid_loss2/points)\n",
    "        valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        # verbose\n",
    "        print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "        print(\"\")\n",
    "    train_fin_losses.append(train_loss)\n",
    "    val_fin_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'{round(i_times/len(lrs),4)*100}% done!')\n",
    "    i_times+=1\n",
    "    \n",
    "    ## Clean GPU\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "    ## Model performance metrics on test set\n",
    "    num_maps=test_loader.dataset.size\n",
    "    ## Now loop over test set and print accuracy\n",
    "    # define the arrays containing the value of the parameters\n",
    "    params_true = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "    params_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "    errors_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "    \n",
    "    end_time_loop = time.time()\n",
    "    time_diff = '(h):', \"{:.4f}\".format((end_time_loop-start_time_loop)/3600.0)\n",
    "    print(f'Loop {i-1} took {time_diff}')\n",
    "    print('Time taken so far in total: (h):', \"{:.4f}\".format((end_time_loop-start)/3600.0))\n",
    "\n",
    "stop = time.time()\n",
    "print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "plt.plot(lrs, train_fin_losses, label = \"Training loss\", linestyle=\"-.\")\n",
    "plt.plot(lrs, val_fin_losses, label = \"Validation Loss\", linestyle=\":\")\n",
    "plt.legend()\n",
    "plt.title(f'{model_size} Model on {data_size} data for {epochs} epochs: {test_parameter}')\n",
    "plt.xlabel(f'{test_parameter}')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#For log_scale learning rate\n",
    "\n",
    "plt.plot(exp, train_fin_losses, label = \"Training loss\", linestyle=\"-.\")\n",
    "plt.plot(exp, val_fin_losses, label = \"Validation Loss\", linestyle=\":\")\n",
    "plt.legend()\n",
    "plt.title(f'{model_size} Model on {data_size} data for {epochs} epochs: {test_parameter}')\n",
    "plt.xlabel('-log(x) (negative of 10^x exponent)')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "X = []\n",
    "Y = []\n",
    "for x,y in betas:\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax.plot(X, Y, val_fin_losses)\n",
    "ax.set_xlabel('Beta1')\n",
    "ax.set_ylabel('Beta2')\n",
    "ax.set_zlabel('Validation Loss')\n",
    "\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax1 = plt.axes(projection='3d')\n",
    "\n",
    "ax1.plot(X, Y, train_fin_losses)\n",
    "ax1.set_xlabel('Beta1')\n",
    "ax1.set_ylabel('Beta2')\n",
    "ax1.set_zlabel('Training Loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(lr,train_fin_losses,label = 'Training Loss', linestyle=\"-.\")\n",
    "plt.plot(lr,val_fin_losses,label = 'Validation Loss', linestyle=\":\")\n",
    "plt.title(f'{model_size} Model with {data_size} data: Learning Rate')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "minpos = val_fin_losses.index(min(val_fin_losses))\n",
    "print('Min Validation is equal to:', val_fin_losses[minpos])\n",
    "print('Which corresponds to a training loss of:', train_fin_losses[minpos])\n",
    "print(f'And is achieved by the {training_parameter} =', lrs[minpos])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "minpos = val_fin_losses.index(min(val_fin_losses))\n",
    "print('Min Validation is equal to:', val_fin_losses[minpos])\n",
    "print('Which corresponds to a training loss of:', train_fin_losses[minpos])\n",
    "print(f'And is achieved by the b1 =',X[minpos], 'and b2 =',Y[minpos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test loss\n",
    "\n",
    "test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "test_loss, points = 0.0, 0\n",
    "model.eval()\n",
    "for x, y in test_loader:\n",
    "    print(points)\n",
    "    with torch.no_grad():\n",
    "        x    = x[:,:,16:-16,16:-16]\n",
    "        bs    = x.shape[0]         #batch size\n",
    "        x     = x.to(device)       #send data to device\n",
    "        y     = y.to(device)[:,g]  #send data to device\n",
    "        p     = model(x)           #prediction for mean and variance\n",
    "        y_NN  = p[:,g]             #prediction for mean\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        if features==4 or features==12:\n",
    "            e_NN  = p[:,h]         #posterior std\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            test_loss2 += loss2*bs\n",
    "        test_loss1 += loss1*bs\n",
    "        test_loss = torch.log(test_loss1/points)\n",
    "        if features==4 or features==12:\n",
    "            test_loss+=torch.log(test_loss2/points)\n",
    "        test_loss = torch.mean(test_loss).item()\n",
    "\n",
    "        # save results to their corresponding arrays\n",
    "        params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "        params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "        if features==4 or features==12:\n",
    "            errors_NN[points:points+x.shape[0]]   = np.abs(e_NN.cpu().numpy())\n",
    "        points    += x.shape[0]\n",
    "test_loss = torch.log(test_loss1/points) + torch.log(test_loss2/points)\n",
    "test_loss = torch.mean(test_loss).item()\n",
    "print('Test loss = %.3e\\n'%test_loss)\n",
    "\n",
    "# de-normalize\n",
    "## I guess these are the hardcoded parameter limits\n",
    "minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "\n",
    "## Drop feedback parameters if they aren't included\n",
    "minimum=minimum[g]\n",
    "maximum=maximum[g]\n",
    "params_true = params_true*(maximum - minimum) + minimum\n",
    "params_NN   = params_NN*(maximum - minimum) + minimum\n",
    "\n",
    "\n",
    "test_error = 100*np.mean(np.sqrt((params_true - params_NN)**2)/params_true,axis=0)\n",
    "print('Error Omega_m = %.3f'%test_error[0])\n",
    "print('Error sigma_8 = %.3f'%test_error[1])\n",
    "\n",
    "wandb.run.summary[\"Error Omega_m\"]=test_error[0]\n",
    "wandb.run.summary[\"Error sigma_8\"]=test_error[1]\n",
    "\n",
    "if features>4:\n",
    "    print('Error A_SN1   = %.3f'%test_error[2])\n",
    "    print('Error A_AGN1  = %.3f'%test_error[3])\n",
    "    print('Error A_SN2   = %.3f'%test_error[4])\n",
    "    print('Error A_AGN2  = %.3f\\n'%test_error[5])\n",
    "    wandb.run.summary[\"Error A_SN1\"]  =test_error[2]\n",
    "    wandb.run.summary[\"Error A_AGN1\"] =test_error[3]\n",
    "    wandb.run.summary[\"Error A_SN2\"]  =test_error[4]\n",
    "    wandb.run.summary[\"Error A_AGN2\"] =test_error[5]\n",
    "\n",
    "wandb.run.summary[\"Error Omega_m\"]=test_error[0]\n",
    "wandb.run.summary[\"Error sigma_8\"]=test_error[1]\n",
    "\n",
    "if features==4:\n",
    "    errors_NN   = errors_NN*(maximum - minimum)\n",
    "    mean_error = 100*(np.absolute(np.mean(errors_NN/params_NN, axis=0)))\n",
    "    print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "    print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "    wandb.run.summary[\"Predicted error Omega_m\"]=mean_error[0]\n",
    "    wandb.run.summary[\"Predicted error sigma_8\"]=mean_error[1]\n",
    "\n",
    "elif features==12:\n",
    "    errors_NN   = errors_NN*(maximum - minimum)\n",
    "    mean_error = 100*(np.absolute(np.mean(errors_NN/params_NN, axis=0)))\n",
    "    print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "    print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "    print('Bayesian error A_SN1   = %.3f'%mean_error[2])\n",
    "    print('Bayesian error A_AGN1  = %.3f'%mean_error[3])\n",
    "    print('Bayesian error A_SN2   = %.3f'%mean_error[4])\n",
    "    print('Bayesian error A_AGN2  = %.3f\\n'%mean_error[5])\n",
    "    wandb.run.summary[\"Predicted error Omega_m\"]=mean_error[0]\n",
    "    wandb.run.summary[\"Predicted error sigma_8\"]=mean_error[1]\n",
    "    wandb.run.summary[\"Predicted error A_SN1\"]  =mean_error[2]\n",
    "    wandb.run.summary[\"Predicted error A_AGN1\"] =mean_error[3]\n",
    "    wandb.run.summary[\"Predicted error A_SN2\"]  =mean_error[4]\n",
    "    wandb.run.summary[\"Predicted error A_AGN2\"] =mean_error[5]\n",
    "\n",
    "\n",
    "if features<5:\n",
    "    f, axarr = plt.subplots(1, 2, figsize=(9,6))\n",
    "    axarr[0].plot(np.linspace(min(params_true[:,0]),max(params_true[:,0]),100),np.linspace(min(params_true[:,0]),max(params_true[:,0]),100),color=\"black\")\n",
    "    axarr[1].plot(np.linspace(min(params_true[:,1]),max(params_true[:,1]),100),np.linspace(min(params_true[:,1]),max(params_true[:,1]),100),color=\"black\")\n",
    "    if features==4:\n",
    "        axarr[0].errorbar(params_true[:,0],params_NN[:,0],errors_NN[:,0],marker=\"o\",ls=\"none\")\n",
    "        axarr[1].errorbar(params_true[:,1],params_NN[:,1],errors_NN[:,1],marker=\"o\",ls=\"none\")\n",
    "    else:\n",
    "        axarr[0].plot(params_true[:,0],params_NN[:,0],marker=\"o\",ls=\"none\")\n",
    "        axarr[1].plot(params_true[:,1],params_NN[:,1],marker=\"o\",ls=\"none\")\n",
    "        \n",
    "    axarr[0].set_xlabel(r\"True $\\Omega_m$\")\n",
    "    axarr[0].set_ylabel(r\"Predicted $\\Omega_m$\")\n",
    "    axarr[0].text(0.1,0.9,\"%.3f %% error\" % test_error[0],fontsize=12,transform=axarr[0].transAxes)\n",
    "\n",
    "    axarr[1].set_xlabel(r\"True $\\sigma_8$\")\n",
    "    axarr[1].set_ylabel(r\"Predicted $\\sigma_8$\")\n",
    "    axarr[1].text(0.1,0.9,\"%.3f %% error\" % test_error[1],fontsize=12,transform=axarr[1].transAxes)\n",
    "\n",
    "\n",
    "if features>4:\n",
    "    f, axarr = plt.subplots(3, 2, figsize=(14,20))\n",
    "    for aa in range(0,6,2):\n",
    "        axarr[aa//2][0].plot(np.linspace(min(params_true[:,aa]),max(params_true[:,aa]),100),np.linspace(min(params_true[:,aa]),max(params_true[:,aa]),100),color=\"black\")\n",
    "        axarr[aa//2][1].plot(np.linspace(min(params_true[:,aa+1]),max(params_true[:,aa+1]),100),np.linspace(min(params_true[:,aa+1]),max(params_true[:,aa+1]),100),color=\"black\")\n",
    "        if features==12:\n",
    "            axarr[aa//2][0].errorbar(params_true[:,aa],params_NN[:,aa],errors_NN[:,aa],marker=\"o\",ls=\"none\")\n",
    "            axarr[aa//2][1].errorbar(params_true[:,aa+1],params_NN[:,aa+1],errors_NN[:,aa+1],marker=\"o\",ls=\"none\")\n",
    "        else:\n",
    "            axarr[aa//2][0].plot(params_true[:,aa],params_NN[:,aa],marker=\"o\",ls=\"none\")\n",
    "            axarr[aa//2][1].plot(params_true[:,aa+1],params_NN[:,aa+1],marker=\"o\",ls=\"none\")\n",
    "            \n",
    "    axarr[0][0].set_xlabel(r\"True $\\Omega_m$\")\n",
    "    axarr[0][0].set_ylabel(r\"Predicted $\\Omega_m$\")\n",
    "    axarr[0][0].text(0.1,0.9,\"%.3f %% error\" % test_error[0],fontsize=12,transform=axarr[0][0].transAxes)\n",
    "\n",
    "    axarr[0][1].set_xlabel(r\"True $\\sigma_8$\")\n",
    "    axarr[0][1].set_ylabel(r\"Predicted $\\sigma_8$\")\n",
    "    axarr[0][1].text(0.1,0.9,\"%.3f %% error\" % test_error[1],fontsize=12,transform=axarr[0][1].transAxes)\n",
    "\n",
    "    axarr[1][0].set_xlabel(r\"True $A_\\mathrm{SN1}$\")\n",
    "    axarr[1][0].set_ylabel(r\"Predicted $A_\\mathrm{SN1}$\")\n",
    "    axarr[1][0].text(0.1,0.9,\"%.3f %% error\" % test_error[2],fontsize=12,transform=axarr[1][0].transAxes)\n",
    "\n",
    "    axarr[1][1].set_xlabel(r\"True $A_\\mathrm{AGN1}$\")\n",
    "    axarr[1][1].set_ylabel(r\"Predicted $A_\\mathrm{AGN1}$\")\n",
    "    axarr[1][1].text(0.1,0.9,\"%.3f %% error\" % test_error[3],fontsize=12,transform=axarr[1][1].transAxes)\n",
    "\n",
    "    axarr[2][0].set_xlabel(r\"True $A_\\mathrm{SN2}$\")\n",
    "    axarr[2][0].set_ylabel(r\"Predicted $A_\\mathrm{SN2}$\")\n",
    "    axarr[2][0].text(0.1,0.9,\"%.3f %% error\" % test_error[4],fontsize=12,transform=axarr[2][0].transAxes)\n",
    "\n",
    "    axarr[2][1].set_xlabel(r\"True $A_\\mathrm{AGN2}$\")\n",
    "    axarr[2][1].set_ylabel(r\"Predicted $A_\\mathrm{AGN2}$\")\n",
    "    axarr[2][1].text(0.1,0.9,\"%.3f %% error\" % test_error[4],fontsize=12,transform=axarr[2][1].transAxes)\n",
    "\n",
    "figure=wandb.Image(f)\n",
    "wandb.log({\"performance\": figure})\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"success_retro.wav\",autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env2",
   "language": "python",
   "name": "my_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "098e687a0ceb167341ba29473143ae147a995faf0ebe438c7d67b022c8c7c5a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
